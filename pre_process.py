import os
import pickle
import struct
import cv2 as cv
import numpy as np
import torch
import torchvision.transforms as transforms
from PIL import Image
from tqdm import tqdm
from io import BytesIO
import json
import time
import shutil
from config import path_imgidx, path_imgrec, IMG_DIR, pickle_file

def ensure_folder(folder):
    """ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨"""
    if not os.path.exists(folder):
        os.makedirs(folder)
        print(f"åˆ›å»ºç›®å½•: {folder}")

class RecordIOReader:
    """é’ˆå¯¹æ–‡æœ¬ç´¢å¼•æ ¼å¼çš„RecordIOè¯»å–å™¨"""
    
    def __init__(self, idx_path, rec_path):
        self.idx_path = idx_path
        self.rec_path = rec_path
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(idx_path):
            raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {idx_path}")
        if not os.path.exists(rec_path):
            raise FileNotFoundError(f"è®°å½•æ–‡ä»¶ä¸å­˜åœ¨: {rec_path}")
            
        self.offsets = self._load_text_indices()
        
    def _load_text_indices(self):
        """åŠ è½½æ–‡æœ¬æ ¼å¼çš„ç´¢å¼•æ–‡ä»¶"""
        offsets = []
        
        print("æ­£åœ¨åŠ è½½ç´¢å¼•æ–‡ä»¶...")
        with open(self.idx_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f):
                line = line.strip()
                if line:
                    try:
                        # ç´¢å¼•æ–‡ä»¶æ ¼å¼: è®°å½•ID\tåç§»é‡
                        parts = line.split('\t')
                        if len(parts) >= 2:
                            offset = int(parts[1])
                            offsets.append(offset)
                    except ValueError as e:
                        print(f"è§£æç¬¬{line_num+1}è¡Œæ—¶å‡ºé”™: {line}, é”™è¯¯: {e}")
                        continue
        
        print(f"æˆåŠŸåŠ è½½ {len(offsets)} ä¸ªåç§»é‡")
        return offsets
        
    def __len__(self):
        return len(self.offsets)
    
    def read_idx(self, idx):
        """è¯»å–æŒ‡å®šç´¢å¼•çš„è®°å½•"""
        if idx <= 0 or idx > len(self.offsets):
            raise IndexError(f"Index {idx} out of range [1, {len(self.offsets)}]")
        
        # è·å–å½“å‰è®°å½•çš„åç§»é‡
        current_offset = self.offsets[idx - 1]
        
        # è®¡ç®—è®°å½•é•¿åº¦
        if idx < len(self.offsets):
            next_offset = self.offsets[idx]
            length = next_offset - current_offset
        else:
            # æœ€åä¸€ä¸ªè®°å½•ï¼Œé•¿åº¦åˆ°æ–‡ä»¶æœ«å°¾
            file_size = os.path.getsize(self.rec_path)
            length = file_size - current_offset
        
        # è¯»å–æ•°æ®
        with open(self.rec_path, 'rb') as f:
            f.seek(current_offset)
            data = f.read(length)
        
        if len(data) == 0:
            raise ValueError(f"è¯»å–æ•°æ®ä¸ºç©ºï¼Œåç§»é‡: {current_offset}, é•¿åº¦: {length}")
            
        return self._unpack_record(data, idx)
    
    def _unpack_record(self, data, idx):
        """è§£åŒ…è®°å½•æ•°æ®"""
        if len(data) < 16:
            raise ValueError(f"è®°å½•æ•°æ®å¤ªçŸ­: {len(data)} å­—èŠ‚")
        
        # æŸ¥æ‰¾JPEGæ–‡ä»¶å¤´ (FF D8 FF)
        jpeg_start = -1
        for i in range(len(data) - 3):
            if data[i] == 0xFF and data[i+1] == 0xD8 and data[i+2] == 0xFF:
                jpeg_start = i
                break
        
        if jpeg_start == -1:
            # æŸ¥æ‰¾PNGå¤´: 89 50 4E 47
            for i in range(len(data) - 4):
                if (data[i] == 0x89 and data[i+1] == 0x50 and 
                    data[i+2] == 0x4E and data[i+3] == 0x47):
                    jpeg_start = i
                    break
        
        if jpeg_start == -1:
            raise ValueError(f"æ‰¾ä¸åˆ°æœ‰æ•ˆçš„å›¾åƒæ•°æ®å¤´ï¼Œè®°å½• {idx}")
        
        # æå–headerå’Œå›¾åƒæ•°æ®
        header_data = data[:jpeg_start] if jpeg_start > 0 else b""
        image_data = data[jpeg_start:]
        
        # è§£æheader
        header = self._parse_header(header_data, idx)
        
        return header, image_data
    
    def _parse_header(self, header_data, idx):
        """è§£æheaderæ•°æ®"""
        class Header:
            def __init__(self, label=0):
                self.label = label
        
        if len(header_data) == 0:
            return Header(idx % 10000)  # ä½¿ç”¨ç´¢å¼•ä½œä¸ºä¸´æ—¶label
            
        # å°è¯•ä»headerä¸­æå–label
        try:
            # æŸ¥æ‰¾headerä¸­çš„æ•°å€¼
            for i in range(0, len(header_data) - 4, 1):
                try:
                    value = struct.unpack('<I', header_data[i:i+4])[0]
                    if 0 <= value <= 1000000:  # åˆç†çš„labelèŒƒå›´
                        return Header(value)
                except:
                    continue
        except:
            pass
        
        # ä½¿ç”¨idxä½œä¸ºfallback label
        return Header(idx % 10000)

class SafeBatchImageExtractor:
    """å®‰å…¨çš„æ‰¹é‡å›¾åƒæå–å™¨ - è§£å†³æ–‡ä»¶æ¶ˆå¤±é—®é¢˜"""
    
    def __init__(self, img_dir, pickle_file):
        self.img_dir = img_dir
        self.pickle_file = pickle_file
        self.progress_file = pickle_file.replace('.pkl', '_progress.json')
        self.backup_file = pickle_file.replace('.pkl', '_backup.pkl')
        
        # ç¡®ä¿pickleæ–‡ä»¶çš„ç›®å½•å­˜åœ¨
        pickle_dir = os.path.dirname(self.pickle_file)
        if pickle_dir:
            ensure_folder(pickle_dir)
        
    def extract_all_images(self, idx_path, rec_path, batch_size=1000, save_interval=1000):
        """æ‰¹é‡æå–æ‰€æœ‰å›¾åƒ - æ›´é¢‘ç¹çš„å®‰å…¨ä¿å­˜"""
        print(f"ç›®æ ‡å›¾åƒç›®å½•: {self.img_dir}")
        print(f"ç›®æ ‡pickleæ–‡ä»¶: {self.pickle_file}")
        print(f"å¤‡ä»½æ–‡ä»¶: {self.backup_file}")
        
        ensure_folder(self.img_dir)
        
        # åˆå§‹åŒ–è¯»å–å™¨
        print("åˆå§‹åŒ–RecordIOè¯»å–å™¨...")
        try:
            reader = RecordIOReader(idx_path, rec_path)
        except Exception as e:
            print(f"æ— æ³•åˆå§‹åŒ–RecordIOè¯»å–å™¨: {e}")
            return []
        
        total_records = len(reader)
        print(f"æ€»è®°å½•æ•°: {total_records}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¹‹å‰çš„è¿›åº¦
        start_idx, samples, class_ids = self._load_progress()
        
        print(f"ä»ç´¢å¼• {start_idx} å¼€å§‹æå–...")
        
        success_count = len(samples)
        error_count = 0
        last_save_time = time.time()
        
        try:
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(total=total_records, initial=start_idx, desc="æå–å›¾åƒ") as pbar:
                for i in range(start_idx, total_records):
                    try:
                        # è¯»å–è®°å½•
                        header, image_data = reader.read_idx(i + 1)
                        
                        # è§£ç å›¾åƒ
                        img = self._decode_image(image_data)
                        if img is None:
                            error_count += 1
                            pbar.update(1)
                            continue
                        
                        # è½¬æ¢ä¸ºOpenCVæ ¼å¼ (BGR)
                        img_cv = cv.cvtColor(np.array(img), cv.COLOR_RGB2BGR)
                        
                        # è·å–æ ‡ç­¾
                        label = int(header.label)
                        class_ids.add(label)
                        
                        # ä¿å­˜å›¾åƒ
                        filename = f'{i}.jpg'
                        filepath = os.path.join(self.img_dir, filename)
                        success_write = cv.imwrite(filepath, img_cv)
                        
                        if not success_write:
                            print(f"è­¦å‘Š: æ— æ³•ä¿å­˜å›¾åƒ {filepath}")
                            error_count += 1
                            pbar.update(1)
                            continue
                        
                        # æ·»åŠ åˆ°æ ·æœ¬åˆ—è¡¨
                        samples.append({'img': filename, 'label': label})
                        success_count += 1
                        
                        # æ›´é¢‘ç¹çš„å®‰å…¨ä¿å­˜ - æ¯1000ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡
                        current_time = time.time()
                        if (i + 1) % save_interval == 0 or current_time - last_save_time > 60:  # æ¯åˆ†é’Ÿä¿å­˜ä¸€æ¬¡
                            print(f"\næ­£åœ¨ä¿å­˜è¿›åº¦... (å·²å¤„ç† {success_count} ä¸ªæ ·æœ¬)")
                            
                            # å®‰å…¨ä¿å­˜
                            self._safe_save_all(i + 1, samples, class_ids)
                            last_save_time = current_time
                            
                            # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                            pbar.set_postfix({
                                'Success': success_count,
                                'Errors': error_count,
                                'Classes': len(class_ids),
                                'Last_Save': 'OK'
                            })
                        
                        pbar.update(1)
                        
                    except Exception as e:
                        error_count += 1
                        if error_count <= 20:  # åªæ‰“å°å‰20ä¸ªé”™è¯¯
                            tqdm.write(f"å¤„ç†ç¬¬{i}ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
                        pbar.update(1)
                        continue
                        
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨å®‰å…¨ä¿å­˜å½“å‰è¿›åº¦...")
            self._safe_save_all(i, samples, class_ids)
            print("âœ… è¿›åº¦å·²å®‰å…¨ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†ä»æ­¤å¤„ç»§ç»­")
            return samples
            
        except Exception as e:
            print(f"\nâŒ æå–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            self._safe_save_all(i, samples, class_ids)
        
        # æœ€ç»ˆä¿å­˜
        print(f"\nğŸ‰ æå–å®Œæˆï¼")
        print(f"æˆåŠŸå¤„ç†: {success_count}")
        print(f"å¤±è´¥: {error_count}")
        if success_count + error_count > 0:
            print(f"æˆåŠŸç‡: {success_count/(success_count+error_count)*100:.2f}%")
        
        # æœ€ç»ˆå®‰å…¨ä¿å­˜
        self._safe_save_all(total_records, samples, class_ids)
        self._print_statistics(samples, class_ids)
        
        # æ¸…ç†è¿›åº¦æ–‡ä»¶
        if os.path.exists(self.progress_file):
            os.remove(self.progress_file)
        
        return samples
    
    def _safe_save_all(self, current_idx, samples, class_ids):
        """å®‰å…¨ä¿å­˜æ‰€æœ‰æ•°æ® - å¤šé‡å¤‡ä»½ç­–ç•¥"""
        try:
            # 1. ä¿å­˜è¿›åº¦æ–‡ä»¶
            self._save_progress(current_idx, samples, class_ids)
            
            # 2. ä¿å­˜ä¸»pickleæ–‡ä»¶
            self._safe_save_pickle(samples, self.pickle_file)
            
            # 3. ä¿å­˜å¤‡ä»½æ–‡ä»¶
            self._safe_save_pickle(samples, self.backup_file)
            
            print(f"âœ… æ•°æ®å·²å®‰å…¨ä¿å­˜ (ä¸»æ–‡ä»¶ + å¤‡ä»½)")
            
        except Exception as e:
            print(f"âŒ å®‰å…¨ä¿å­˜å¤±è´¥: {e}")
    
    def _safe_save_pickle(self, samples, filepath):
        """å®‰å…¨ä¿å­˜pickleæ–‡ä»¶ - ç›´æ¥å†™å…¥ï¼Œç«‹å³åˆ·æ–°"""
        try:
            print(f"ä¿å­˜ {len(samples)} ä¸ªæ ·æœ¬åˆ° {filepath}")
            
            # ç›´æ¥å†™å…¥ï¼Œä¸ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
            with open(filepath, 'wb') as f:
                pickle.dump(samples, f)
                f.flush()  # å¼ºåˆ¶åˆ·æ–°åˆ°ç£ç›˜
                os.fsync(f.fileno())  # ç¡®ä¿å†™å…¥ç£ç›˜
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨ä¸”æœ‰å†…å®¹
            if os.path.exists(filepath):
                size = os.path.getsize(filepath)
                print(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ: {filepath} ({size} å­—èŠ‚)")
            else:
                print(f"âŒ è­¦å‘Š: æ–‡ä»¶ä¿å­˜åä¸å­˜åœ¨: {filepath}")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜pickleå¤±è´¥: {e}")
            # å¦‚æœä¿å­˜å¤±è´¥ï¼Œè‡³å°‘ä¿å­˜ä¸€ä¸ªJSONç‰ˆæœ¬ä½œä¸ºå¤‡ç”¨
            try:
                json_file = filepath.replace('.pkl', '.json')
                with open(json_file, 'w') as f:
                    json.dump(samples[:1000], f)  # åªä¿å­˜å‰1000ä¸ªæ ·æœ¬çš„JSON
                print(f"âœ… å¤‡ç”¨JSONæ–‡ä»¶å·²ä¿å­˜: {json_file}")
            except:
                pass
    
    def _load_progress(self):
        """åŠ è½½ä¹‹å‰çš„è¿›åº¦"""
        # ä¼˜å…ˆä»ä¸»æ–‡ä»¶åŠ è½½
        for pickle_path in [self.pickle_file, self.backup_file]:
            if os.path.exists(pickle_path):
                try:
                    with open(pickle_path, 'rb') as f:
                        samples = pickle.load(f)
                    print(f"âœ… ä» {pickle_path} åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
                    
                    # ä»è¿›åº¦æ–‡ä»¶è·å–å¼€å§‹ç´¢å¼•
                    start_idx = 0
                    if os.path.exists(self.progress_file):
                        try:
                            with open(self.progress_file, 'r') as f:
                                progress = json.load(f)
                            start_idx = progress.get('last_idx', len(samples))
                        except:
                            start_idx = len(samples)
                    else:
                        start_idx = len(samples)
                    
                    # é‡å»ºclass_ids
                    class_ids = set([s['label'] for s in samples])
                    
                    print(f"ç»§ç»­ä»ç´¢å¼• {start_idx} å¼€å§‹")
                    return start_idx, samples, class_ids
                    
                except Exception as e:
                    print(f"åŠ è½½ {pickle_path} å¤±è´¥: {e}")
                    continue
        
        print("æ²¡æœ‰æ‰¾åˆ°ä¹‹å‰çš„è¿›åº¦ï¼Œä»å¤´å¼€å§‹")
        return 0, [], set()
    
    def _save_progress(self, current_idx, samples, class_ids):
        """ä¿å­˜å½“å‰è¿›åº¦"""
        progress = {
            'last_idx': current_idx,
            'total_samples': len(samples),
            'total_classes': len(class_ids),
            'timestamp': time.time(),
            'pickle_file': self.pickle_file,
            'backup_file': self.backup_file
        }
        
        try:
            with open(self.progress_file, 'w') as f:
                json.dump(progress, f, indent=2)
                f.flush()
                os.fsync(f.fileno())
        except Exception as e:
            print(f"ä¿å­˜è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
    
    def _decode_image(self, image_data):
        """è§£ç å›¾åƒæ•°æ®"""
        if len(image_data) < 10:
            return None
            
        try:
            # ä½¿ç”¨PILè§£ç 
            img = Image.open(BytesIO(image_data))
            img = img.convert('RGB')
            return img
        except:
            try:
                # ä½¿ç”¨OpenCVè§£ç 
                img_array = np.frombuffer(image_data, dtype=np.uint8)
                img_cv = cv.imdecode(img_array, cv.IMREAD_COLOR)
                if img_cv is not None and img_cv.size > 0:
                    img_rgb = cv.cvtColor(img_cv, cv.COLOR_BGR2RGB)
                    return Image.fromarray(img_rgb)
            except:
                pass
        return None
    
    def _print_statistics(self, samples, class_ids):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f'\n=== ğŸ“Š æœ€ç»ˆç»Ÿè®¡ ===')
        print(f'æ€»æ ·æœ¬æ•°: {len(samples)}')
        print(f'ç±»åˆ«æ•°: {len(class_ids)}')
        
        if class_ids:
            class_ids_list = list(class_ids)
            print(f'æœ€å¤§ç±»åˆ«ID: {max(class_ids_list)}')
            print(f'æœ€å°ç±»åˆ«ID: {min(class_ids_list)}')
        
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        print(f'\n=== ğŸ“ æ–‡ä»¶éªŒè¯ ===')
        for file_path in [self.pickle_file, self.backup_file]:
            if os.path.exists(file_path):
                size = os.path.getsize(file_path)
                print(f'âœ… {file_path}: {size} å­—èŠ‚')
            else:
                print(f'âŒ {file_path}: ä¸å­˜åœ¨')

def main():
    """ä¸»å‡½æ•° - å®‰å…¨æ‰¹é‡æå–"""
    
    print("=== ğŸš€ å®‰å…¨æ‰¹é‡å›¾åƒæå–å™¨ ===")
    print(f"ç›®æ ‡ç›®å½•: {IMG_DIR}")
    print(f"å…ƒæ•°æ®æ–‡ä»¶: {pickle_file}")
    
    # æ£€æŸ¥é…ç½®
    print(f"\n=== ğŸ” é…ç½®æ£€æŸ¥ ===")
    print(f"ç´¢å¼•æ–‡ä»¶: {path_imgidx}")
    print(f"è®°å½•æ–‡ä»¶: {path_imgrec}")
    print(f"ç´¢å¼•æ–‡ä»¶å­˜åœ¨: {os.path.exists(path_imgidx)}")
    print(f"è®°å½•æ–‡ä»¶å­˜åœ¨: {os.path.exists(path_imgrec)}")
    
    if not os.path.exists(path_imgidx):
        print(f"âŒ é”™è¯¯: ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {path_imgidx}")
        return
    
    if not os.path.exists(path_imgrec):
        print(f"âŒ é”™è¯¯: è®°å½•æ–‡ä»¶ä¸å­˜åœ¨: {path_imgrec}")
        return
    
    # åˆ›å»ºå®‰å…¨æå–å™¨
    extractor = SafeBatchImageExtractor(IMG_DIR, pickle_file)
    
    # å¼€å§‹æ‰¹é‡æå– - æ›´é¢‘ç¹çš„ä¿å­˜
    print("\nğŸƒâ€â™‚ï¸ å¼€å§‹å®‰å…¨æ‰¹é‡æå–...")
    print("ğŸ’¡ æç¤º: æ–‡ä»¶æ¯1000ä¸ªæ ·æœ¬è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡ï¼ŒCtrl+C å¯å®‰å…¨ä¸­æ–­")
    
    samples = extractor.extract_all_images(
        path_imgidx, 
        path_imgrec,
        batch_size=1000,      # æ‰¹æ¬¡å¤§å°
        save_interval=1000    # æ¯1000ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ (æ›´é¢‘ç¹)
    )
    
    if len(samples) > 0:
        print(f"\nğŸ‰ æ‰¹é‡æå–å®Œæˆï¼æ€»å…±æå–äº† {len(samples)} ä¸ªæ ·æœ¬")
        
        # æœ€ç»ˆéªŒè¯
        for file_path in [pickle_file, pickle_file.replace('.pkl', '_backup.pkl')]:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'rb') as f:
                        test_samples = pickle.load(f)
                    print(f"âœ… {file_path} éªŒè¯æˆåŠŸï¼ŒåŒ…å« {len(test_samples)} ä¸ªæ ·æœ¬")
                except Exception as e:
                    print(f"âŒ {file_path} éªŒè¯å¤±è´¥: {e}")
            else:
                print(f"âŒ è­¦å‘Š: {file_path} ä¸å­˜åœ¨")
                
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•æ ·æœ¬")

if __name__ == "__main__":
    main()